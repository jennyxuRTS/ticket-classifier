{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60a1a8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JennyXu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JennyXu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8359101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data packages...\n",
      "Downloads complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JennyXu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JennyXu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JennyXu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell first to download the necessary NLTK data\n",
    "import nltk\n",
    "print(\"Downloading NLTK data packages...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "print(\"Downloads complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "957cf27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JennyXu\\AppData\\Local\\Temp\\ipykernel_29280\\2003476494.py:5: DtypeWarning: Columns (22,27,28,29,30,31,34,35,36,39,40,44,50,51,53,54,55,56,57,60,62,63,64,65,66,68,69,70,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,193,194,195,196,197,198,199,213,214,215,218,219,220,224,234,235,237,269,270,314,316,317,318,319,384,385,386,387,388,389,390,391,393,396,428,439,440,441,442,443,444,445,446,447,448,505,529,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lspi_jira_raw = pd.read_csv(lspi_jira_csv_file_path)\n"
     ]
    }
   ],
   "source": [
    "# read data into dataframe\n",
    "\n",
    "lspi_jira_csv_file_path = Path('data/lspi-jira-ticket-data.csv')\n",
    "\n",
    "lspi_jira_raw = pd.read_csv(lspi_jira_csv_file_path)\n",
    "lspi_jira_raw = lspi_jira_raw.iloc[:, :49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad822440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and combining 'Summary' and 'Description' columns...\n",
      "Consolidating label columns...\n",
      "Label consolidation complete.\n",
      "Exporting 'no_label' data to files...\n",
      "Found 1024 rows with 'no_label'. Splitting into up to 5 files with max 205 rows each.\n",
      "Successfully saved 205 rows to no_label_chunk_1.txt\n",
      "Successfully saved 205 rows to no_label_chunk_2.txt\n",
      "Successfully saved 205 rows to no_label_chunk_3.txt\n",
      "Successfully saved 205 rows to no_label_chunk_4.txt\n",
      "Successfully saved 204 rows to no_label_chunk_5.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import contractions\n",
    "import math\n",
    "\n",
    "# This script assumes you have a pandas DataFrame named 'lspi_jira_raw' already loaded in your environment.\n",
    "\n",
    "# --- 1. The Simpler, NLTK-Free Cleaning Function ---\n",
    "def simple_clean_text(text):\n",
    "    \"\"\"\n",
    "    A simplified cleaning function that does NOT use NLTK.\n",
    "    It focuses on removing machine-generated noise, which is ideal for LLM pre-processing.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\[\\^.*?\\]', '', text)\n",
    "    text = re.sub(r'\\[\\~accountid:\\w+\\]', '', text)\n",
    "    text = re.sub(r'\\{color:.*?\\}', '', text)\n",
    "    text = re.sub(r'\\[.*?\\|.*?\\]', '', text)\n",
    "    text = re.sub(r'\\!.*?\\!', '', text)\n",
    "    text = re.sub(r'h\\d\\.', '', text)\n",
    "    text = re.sub(r'\\{[^{}]*\\}', '', text, flags=re.DOTALL)\n",
    "    text = contractions.fix(text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# --- 2. Apply Text Cleaning and Combine Text Columns ---\n",
    "print(\"Cleaning and combining 'Summary' and 'Description' columns...\")\n",
    "lspi_jira_raw['Summary'] = lspi_jira_raw['Summary'].astype(str).fillna('')\n",
    "lspi_jira_raw['Description'] = lspi_jira_raw['Description'].astype(str).fillna('')\n",
    "lspi_jira_raw['Cleaned_Summary'] = lspi_jira_raw['Summary'].apply(simple_clean_text)\n",
    "lspi_jira_raw['Cleaned_Description'] = lspi_jira_raw['Description'].apply(simple_clean_text)\n",
    "lspi_jira_raw['Combined_Text'] = (lspi_jira_raw['Cleaned_Summary'] + ' ' + lspi_jira_raw['Cleaned_Description']).str.strip()\n",
    "\n",
    "# --- 3. Consolidate Label Columns ---\n",
    "print(\"Consolidating label columns...\")\n",
    "# Dynamically find all columns that start with 'Labels'\n",
    "label_cols = [col for col in lspi_jira_raw.columns if col.startswith('Labels')]\n",
    "\n",
    "if label_cols:\n",
    "    # Apply a function row-wise to combine the labels\n",
    "    lspi_jira_raw['Consolidated_Labels'] = lspi_jira_raw.apply(\n",
    "        lambda row: ', '.join(row[label_cols].dropna().astype(str).unique()),\n",
    "        axis=1\n",
    "    )\n",
    "    # Replace any empty strings in the new column with 'no_label'\n",
    "    lspi_jira_raw.loc[lspi_jira_raw['Consolidated_Labels'] == '', 'Consolidated_Labels'] = 'no_label'\n",
    "    print(\"Label consolidation complete.\")\n",
    "else:\n",
    "    print(\"Warning: No columns starting with 'Labels' were found. Skipping label consolidation.\")\n",
    "    # Create the column with 'no_label' for all rows so the next step doesn't fail\n",
    "    lspi_jira_raw['Consolidated_Labels'] = 'no_label'\n",
    "\n",
    "\n",
    "# --- 4. Filter, Chunk, and Save 'no_label' Data to Files ---\n",
    "print(\"Exporting 'no_label' data to files...\")\n",
    "\n",
    "# Step 4.1: Filter for 'no_label' rows and get the text\n",
    "no_label_df = lspi_jira_raw[lspi_jira_raw['Consolidated_Labels'] == 'no_label']\n",
    "texts_to_save = no_label_df['Combined_Text']\n",
    "\n",
    "# Step 4.2: Check if there is anything to save\n",
    "if not texts_to_save.empty:\n",
    "    # Step 4.3: Calculate chunk size\n",
    "    num_files = 5\n",
    "    total_rows = len(texts_to_save)\n",
    "    # Use math.ceil to ensure all rows are included, even with remainders\n",
    "    chunk_size = math.ceil(total_rows / num_files)\n",
    "    print(f\"Found {total_rows} rows with 'no_label'. Splitting into up to {num_files} files with max {chunk_size} rows each.\")\n",
    "\n",
    "    # Step 4.4: Loop to create each file\n",
    "    for i in range(num_files):\n",
    "        # Define the start and end index for the chunk\n",
    "        start_index = i * chunk_size\n",
    "        end_index = start_index + chunk_size\n",
    "        \n",
    "        # Get the chunk of text\n",
    "        chunk = texts_to_save.iloc[start_index:end_index]\n",
    "        \n",
    "        # If the chunk is empty (e.g., we have fewer rows than chunks), stop.\n",
    "        if chunk.empty:\n",
    "            break\n",
    "            \n",
    "        # Define the filename\n",
    "        filename = f\"no_label_chunk_{i+1}.txt\"\n",
    "        \n",
    "        # Write the chunk to a text file\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            # Join all text entries in the chunk with two newlines for separation\n",
    "            file_content = '\\n\\n'.join(chunk)\n",
    "            f.write(file_content)\n",
    "            \n",
    "        print(f\"Successfully saved {len(chunk)} rows to {filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"No rows with 'no_label' were found to export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6889537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 5 files into combined.csv with 1024 rows.\n"
     ]
    }
   ],
   "source": [
    "# combine labeled ticket csvs\n",
    "\n",
    "files = [\n",
    "    'jira-tickets-labeled/ticket_labels.csv',\n",
    "    'jira-tickets-labeled/ticket_labels_2.csv',\n",
    "    'jira-tickets-labeled/ticket_labels_3.csv',\n",
    "    'jira-tickets-labeled/ticket_labels_4.csv',\n",
    "    'jira-tickets-labeled/ticket_labels_5.csv'\n",
    "]\n",
    "\n",
    "master_df = pd.read_csv(files[0])\n",
    "master_cols = list(master_df.columns)\n",
    "\n",
    "dfs = [master_df]\n",
    "\n",
    "for f in files[1:]:\n",
    "    df = pd.read_csv(f)\n",
    "    if list(df.columns) != master_cols:\n",
    "        raise ValueError(f\"Column mismatch in {f}: {df.columns.tolist()} vs {master_cols}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('combined.csv', index=False)\n",
    "print(f\"Combined {len(files)} files into combined.csv with {len(combined_df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0ea23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
